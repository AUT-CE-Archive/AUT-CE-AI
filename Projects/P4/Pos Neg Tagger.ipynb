{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "regional-sleeping",
   "metadata": {},
   "source": [
    "# Pos Neg Tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-cemetery",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "stone-welding",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import random as rand\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")   # Load language model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mounted-practice",
   "metadata": {},
   "source": [
    "## Stage 1 - Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "latest-globe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list, neg_list = None, None\n",
    "\n",
    "with open('Datasets/Raw/rt-polarity.pos', 'r') as pos_reader:\n",
    "    pos_list = pos_reader.readlines()\n",
    "    \n",
    "with open('Datasets/Raw/rt-polarity.neg', 'r') as neg_reader:\n",
    "    neg_list = neg_reader.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-oriental",
   "metadata": {},
   "source": [
    "### Create DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "mediterranean-wrestling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_sentence</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the rock is destined to be the 21st century's ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the gorgeously elaborate continuation of \" the...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>effective but too-tepid biopic\\n</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>if you sometimes like to go to the movies to h...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>emerges as something rare , an issue movie tha...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5326</th>\n",
       "      <td>a terrible movie that some people will neverth...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5327</th>\n",
       "      <td>there are many definitions of 'time waster' bu...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5328</th>\n",
       "      <td>as it stands , crocodile hunter has the hurrie...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5329</th>\n",
       "      <td>the thing looks like a made-for-home-video qui...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5330</th>\n",
       "      <td>enigma is well-made , but it's just too dry an...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10662 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           raw_sentence       tag\n",
       "0     the rock is destined to be the 21st century's ...  positive\n",
       "1     the gorgeously elaborate continuation of \" the...  positive\n",
       "2                      effective but too-tepid biopic\\n  positive\n",
       "3     if you sometimes like to go to the movies to h...  positive\n",
       "4     emerges as something rare , an issue movie tha...  positive\n",
       "...                                                 ...       ...\n",
       "5326  a terrible movie that some people will neverth...  negative\n",
       "5327  there are many definitions of 'time waster' bu...  negative\n",
       "5328  as it stands , crocodile hunter has the hurrie...  negative\n",
       "5329  the thing looks like a made-for-home-video qui...  negative\n",
       "5330  enigma is well-made , but it's just too dry an...  negative\n",
       "\n",
       "[10662 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_df = pd.DataFrame(data = {'raw_sentence': pos_list, 'tag': ['positive' for _ in pos_list]})\n",
    "neg_df = pd.DataFrame(data = {'raw_sentence': neg_list, 'tag': ['negative' for _ in neg_list]})\n",
    "\n",
    "# Combine pos_df & neg_df\n",
    "comments_df = pos_df.append(neg_df)\n",
    "comments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "described-british",
   "metadata": {},
   "source": [
    "### Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "israeli-auditor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(text):\n",
    "\n",
    "    # Remove leading whitespaces, then convert to Spacy Doc object\n",
    "    doc = nlp(text.strip())\n",
    "    \n",
    "    # Remove punctuations & lemmatize\n",
    "    stopwords = nlp.Defaults.stop_words\n",
    "    text = ' '.join([token.lemma_ for token in doc if not token.is_punct])\n",
    "    \n",
    "    # Manual mapping\n",
    "    text = text.replace('n\\'t', 'not')\n",
    "\n",
    "    # Remove other punctuations\n",
    "    return re.sub('\\'s', '', text)\n",
    "\n",
    "# Clean sentences\n",
    "comments_df['cleaned_sentence'] = comments_df['raw_sentence'].apply(clean_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "artificial-mailman",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comments_df.to_csv('Datasets/Processed/comments_df.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-engagement",
   "metadata": {},
   "source": [
    "## Stage 2 - Unigrams & Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "intellectual-hopkins",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_sentence</th>\n",
       "      <th>tag</th>\n",
       "      <th>cleaned_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the rock is destined to be the 21st century's ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>the rock be destine to be the 21st century  ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the gorgeously elaborate continuation of \" the...</td>\n",
       "      <td>positive</td>\n",
       "      <td>the gorgeously elaborate continuation of the l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>effective but too-tepid biopic\\n</td>\n",
       "      <td>positive</td>\n",
       "      <td>effective but too tepid biopic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>if you sometimes like to go to the movies to h...</td>\n",
       "      <td>positive</td>\n",
       "      <td>if you sometimes like to go to the movie to ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>emerges as something rare , an issue movie tha...</td>\n",
       "      <td>positive</td>\n",
       "      <td>emerge as something rare an issue movie that b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10657</th>\n",
       "      <td>a terrible movie that some people will neverth...</td>\n",
       "      <td>negative</td>\n",
       "      <td>a terrible movie that some people will neverth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10658</th>\n",
       "      <td>there are many definitions of 'time waster' bu...</td>\n",
       "      <td>negative</td>\n",
       "      <td>there be many definition of time waster but th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10659</th>\n",
       "      <td>as it stands , crocodile hunter has the hurrie...</td>\n",
       "      <td>negative</td>\n",
       "      <td>as it stand crocodile hunter have the hurry ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10660</th>\n",
       "      <td>the thing looks like a made-for-home-video qui...</td>\n",
       "      <td>negative</td>\n",
       "      <td>the thing look like a make for home video quickie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10661</th>\n",
       "      <td>enigma is well-made , but it's just too dry an...</td>\n",
       "      <td>negative</td>\n",
       "      <td>enigma be well make but it be just too dry and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10662 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            raw_sentence       tag  \\\n",
       "0      the rock is destined to be the 21st century's ...  positive   \n",
       "1      the gorgeously elaborate continuation of \" the...  positive   \n",
       "2                       effective but too-tepid biopic\\n  positive   \n",
       "3      if you sometimes like to go to the movies to h...  positive   \n",
       "4      emerges as something rare , an issue movie tha...  positive   \n",
       "...                                                  ...       ...   \n",
       "10657  a terrible movie that some people will neverth...  negative   \n",
       "10658  there are many definitions of 'time waster' bu...  negative   \n",
       "10659  as it stands , crocodile hunter has the hurrie...  negative   \n",
       "10660  the thing looks like a made-for-home-video qui...  negative   \n",
       "10661  enigma is well-made , but it's just too dry an...  negative   \n",
       "\n",
       "                                        cleaned_sentence  \n",
       "0      the rock be destine to be the 21st century  ne...  \n",
       "1      the gorgeously elaborate continuation of the l...  \n",
       "2                         effective but too tepid biopic  \n",
       "3      if you sometimes like to go to the movie to ha...  \n",
       "4      emerge as something rare an issue movie that b...  \n",
       "...                                                  ...  \n",
       "10657  a terrible movie that some people will neverth...  \n",
       "10658  there be many definition of time waster but th...  \n",
       "10659  as it stand crocodile hunter have the hurry ba...  \n",
       "10660  the thing look like a make for home video quickie  \n",
       "10661  enigma be well make but it be just too dry and...  \n",
       "\n",
       "[10662 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df = pd.read_csv('Datasets/Processed/comments_df.csv')\n",
    "comments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "according-fairy",
   "metadata": {},
   "source": [
    "### Word Counts & Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "another-recall",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_unigrams, neg_unigrams, pos_bigrams, neg_bigrams = [], [], [], []\n",
    "\n",
    "for index, row in comments_df.iterrows():\n",
    "    \n",
    "    comment = row['cleaned_sentence']\n",
    "    tag = row['tag']\n",
    "    \n",
    "    if pd.isnull(comment):\n",
    "        break\n",
    "    \n",
    "    # Unigrams\n",
    "    if tag == 'positive':\n",
    "        pos_unigrams += comment.split(' ')\n",
    "    else:\n",
    "        neg_unigrams += comment.split(' ')\n",
    "    \n",
    "    # bigrams\n",
    "    if tag == 'positive':\n",
    "        pos_bigrams += [' '.join(bigram) for bigram in zip(comment.split(\" \")[:-1], comment.split(\" \")[1:])]\n",
    "    else:\n",
    "        neg_bigrams += [' '.join(bigram) for bigram in zip(comment.split(\" \")[:-1], comment.split(\" \")[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sharp-barrel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get count and store as dictionary\n",
    "pos_unigrams_dict = pd.Series(pos_unigrams)[10:-10].value_counts().to_dict()\n",
    "neg_unigrams_dict = pd.Series(neg_unigrams)[10:-10].value_counts().to_dict()\n",
    "\n",
    "pos_bigrams_dict = pd.Series(pos_bigrams)[10:-10].value_counts().to_dict()\n",
    "neg_bigrams_dict = pd.Series(neg_bigrams)[10:-10].value_counts().to_dict()\n",
    "\n",
    "# Save JSONs\n",
    "with open('Datasets/Processed/vocab.json', 'w') as write:\n",
    "    output = {\n",
    "        'pos_unigrams_dict':pos_unigrams_dict,\n",
    "        'neg_unigrams_dict': neg_unigrams_dict,\n",
    "        'pos_bigrams_dict': pos_bigrams_dict,\n",
    "        'neg_bigrams_dict': neg_bigrams_dict\n",
    "    }\n",
    "    \n",
    "    json.dump(output, write)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-journey",
   "metadata": {},
   "source": [
    "## Stage 3 - Test & Debug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-substitute",
   "metadata": {},
   "source": [
    "### Calculate probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "certain-museum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSONs\n",
    "with open('Datasets/Processed/vocab.json', 'r') as read:\n",
    "    vocab = json.load(read)\n",
    "\n",
    "pos_unigrams, neg_unigrams, pos_bigrams, neg_bigrams = [vocab[key] for key in vocab.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "organizational-athletics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_prob(word, unigrams):\n",
    "    ''' Calculates probability of the unigram '''\n",
    "    \n",
    "    # Just in case\n",
    "    if word not in unigrams:\n",
    "        return 0\n",
    "        \n",
    "    word_count = unigrams[word]\n",
    "    unigram_size = len(unigrams)\n",
    "    return word_count / unigram_size * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "surprising-charles",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_prob(first_word, second_word, alpha, beta, gamma, coefficient, unigrams, bigrams):\n",
    "    ''' Calculates probability of the bigram '''\n",
    "    \n",
    "    bigram = ' '.join([first_word, second_word])\n",
    "    \n",
    "    # Alpha prob\n",
    "    if bigram not in bigrams:\n",
    "        alpha_prob = 0\n",
    "    else:\n",
    "        bigram_count = bigrams[bigram]\n",
    "        alpha_prob = alpha * (bigram_count / unigrams[first_word]) * 10\n",
    "    \n",
    "    # Beta prob\n",
    "    beta_prob = beta * unigram_prob(second_word, unigrams)\n",
    "    \n",
    "    # coefficient\n",
    "    coef = gamma * coefficient * 10\n",
    "    \n",
    "    return alpha_prob + beta_prob + coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "prospective-change",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_prob(sentence, alpha, beta, gamma, coefficient, unigrams, bigrams):\n",
    "    ''' Calculates probability of the sentence '''\n",
    "    \n",
    "    # Clean & get sentence tokens\n",
    "    cleaned_sentence = clean_sentence(sentence)\n",
    "    words = cleaned_sentence.split(' ')\n",
    "    \n",
    "    # Calculate the first unigram prob\n",
    "    probability = unigram_prob(words[0], unigrams)\n",
    "    \n",
    "    bigrams = [bigram for bigram in zip(cleaned_sentence.split(\" \")[:-1], cleaned_sentence.split(\" \")[1:])]\n",
    "    for bigram in bigrams:\n",
    "        probability *= bigram_prob(bigram[0], bigram[1], alpha, beta, gamma, coefficient, unigrams, bigrams)\n",
    "        \n",
    "    return round(probability, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "educated-welsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(sentence, alpha, beta, gamma, coefficient, pos_unigrams, neg_unigrams, pos_bigrams, neg_bigrams):\n",
    "    ''' Retusn sentiment of the sentence '''\n",
    "    \n",
    "    pos_prob = sentence_prob(sentence, alpha, beta, gamma, coefficient, pos_unigrams, pos_bigrams)\n",
    "    neg_prob = sentence_prob(sentence, alpha, beta, gamma, coefficient, neg_unigrams, neg_bigrams)\n",
    "    \n",
    "    sentiment = 'POSITIVE' if (pos_prob > neg_prob) else 'NEGATIVE'\n",
    "    \n",
    "    return sentiment, pos_prob, neg_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "amended-sunrise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " !q\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    \n",
    "    comment = input()\n",
    "    \n",
    "    if comment == '!q':\n",
    "        break\n",
    "    \n",
    "    sentiment = get_sentiment(comment, 0.4, 0.3, 0.3, 0.5, pos_unigrams, neg_unigrams, pos_bigrams, neg_bigrams)\n",
    "    print(sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "rapid-scanner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ POSITIVE | POS: 0.264763386\t NEG: 0.1385596368\n",
      "- NEGATIVE | POS: 0.0020942766\t NEG: 0.004196811\n",
      "+ POSITIVE | POS: 0.0484180103\t NEG: 0.0391477328\n",
      "- NEGATIVE | POS: 0.4519219794\t NEG: 0.5773533558\n",
      "+ POSITIVE | POS: 0.0782496687\t NEG: 0.0629112068\n",
      "- NEGATIVE | POS: 0.0011839071\t NEG: 0.0022096209\n",
      "+ POSITIVE | POS: 0.0865648545\t NEG: 0.0557123813\n",
      "+ POSITIVE | POS: 0.153699358\t NEG: 0.1493484698\n"
     ]
    }
   ],
   "source": [
    "test_case = [\n",
    "    'loved the movie',                                        # POSITIVE\n",
    "    'Terrible movie, totally hate it.',                       # NEGATIVE\n",
    "    'What a waste of time watching this terible movie',       # NEGATIVE    \n",
    "    'It was fun watching the movie. I was amazing for kids',  # POSITIVE\n",
    "    'Good movie I really like it',                            # POSITIVE\n",
    "    'Terrible plot, come one man',                            # NEGATIVE\n",
    "    'good actors, well played.',                              # POSITIVE    \n",
    "]\n",
    "\n",
    "for test in test_case:\n",
    "    sentiment, pos_prob, neg_prob = get_sentiment(test, 0.5, 0.3, 0.2, 0.29, pos_unigrams, neg_unigrams, pos_bigrams, neg_bigrams)\n",
    "    print('+' if sentiment == 'POSITIVE' else '-', f'{sentiment} | POS: {pos_prob}\\t NEG: {neg_prob}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleasant-cruise",
   "metadata": {},
   "source": [
    "## Finding best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "continued-ivory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tune_test_case = [\n",
    "#     'I like the movie',\n",
    "# ]\n",
    "\n",
    "# while True:\n",
    "    \n",
    "#     r = [rand.random() for i in range(3)]\n",
    "#     s = sum(r)\n",
    "#     alpha, beta, gamma = [ i/s for i in r]\n",
    "    \n",
    "#     coeffitient = rand.random()\n",
    "    \n",
    "#     sentiment, pos_prob, neg_prob = get_sentiment(tune_test_case[0], alpha, beta, gamma, coeffitient, pos_unigrams, neg_unigrams, pos_bigrams, neg_bigrams)\n",
    "    \n",
    "#     if sentiment == 'POSITIVE':\n",
    "#         print('Best Parametes:', alpha, beta, gamma, coeffitient)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subject-runner",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civil-pricing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-final",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
