{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dress-chorus",
   "metadata": {},
   "source": [
    "# Pos Neg Tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungry-chicago",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "latin-dressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")   # Load language model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-locking",
   "metadata": {},
   "source": [
    "## Stage 1 - Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "verbal-complex",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list, neg_list = None, None\n",
    "\n",
    "with open('Datasets/Raw/rt-polarity.pos', 'r') as pos_reader:\n",
    "    pos_list = pos_reader.readlines()\n",
    "    \n",
    "with open('Datasets/Raw/rt-polarity.neg', 'r') as neg_reader:\n",
    "    neg_list = neg_reader.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-sponsorship",
   "metadata": {},
   "source": [
    "### Create DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "heated-humanitarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_df = pd.DataFrame(data = {'raw_sentence': pos_list, 'tag': ['positive' for _ in pos_list]})\n",
    "neg_df = pd.DataFrame(data = {'raw_sentence': neg_list, 'tag': ['negative' for _ in neg_list]})\n",
    "\n",
    "# Combine pos_df & neg_df\n",
    "comments_df = pos_df.append(neg_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funny-columbia",
   "metadata": {},
   "source": [
    "### Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "narrative-infrastructure",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(text):\n",
    "\n",
    "    # Remove leading whitespaces, then convert to Spacy Doc object\n",
    "    doc = nlp(text.strip())\n",
    "    \n",
    "    # Remove punctuations & lemmatize\n",
    "    stopwords = nlp.Defaults.stop_words\n",
    "    text = ' '.join([token.lemma_ for token in doc if not token.is_punct])\n",
    "    \n",
    "    # Manual mapping\n",
    "    text = text.replace('n\\'t', 'not')\n",
    "\n",
    "    # Remove other punctuations\n",
    "    return re.sub('\\'s', '', text)\n",
    "\n",
    "# Clean sentences\n",
    "comments_df['cleaned_sentence'] = comments_df['raw_sentence'].apply(clean_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "swedish-machine",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "comments_df.to_csv('Datasets/Processed/comments_df.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-staff",
   "metadata": {},
   "source": [
    "## Stage 2 - Unigrams & Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "plain-anchor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_sentence</th>\n",
       "      <th>tag</th>\n",
       "      <th>cleaned_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the rock is destined to be the 21st century's ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>the rock be destine to be the 21st century  ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the gorgeously elaborate continuation of \" the...</td>\n",
       "      <td>positive</td>\n",
       "      <td>the gorgeously elaborate continuation of the l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>effective but too-tepid biopic\\n</td>\n",
       "      <td>positive</td>\n",
       "      <td>effective but too tepid biopic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>if you sometimes like to go to the movies to h...</td>\n",
       "      <td>positive</td>\n",
       "      <td>if you sometimes like to go to the movie to ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>emerges as something rare , an issue movie tha...</td>\n",
       "      <td>positive</td>\n",
       "      <td>emerge as something rare an issue movie that b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10657</th>\n",
       "      <td>a terrible movie that some people will neverth...</td>\n",
       "      <td>negative</td>\n",
       "      <td>a terrible movie that some people will neverth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10658</th>\n",
       "      <td>there are many definitions of 'time waster' bu...</td>\n",
       "      <td>negative</td>\n",
       "      <td>there be many definition of time waster but th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10659</th>\n",
       "      <td>as it stands , crocodile hunter has the hurrie...</td>\n",
       "      <td>negative</td>\n",
       "      <td>as it stand crocodile hunter have the hurry ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10660</th>\n",
       "      <td>the thing looks like a made-for-home-video qui...</td>\n",
       "      <td>negative</td>\n",
       "      <td>the thing look like a make for home video quickie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10661</th>\n",
       "      <td>enigma is well-made , but it's just too dry an...</td>\n",
       "      <td>negative</td>\n",
       "      <td>enigma be well make but it be just too dry and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10662 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            raw_sentence       tag  \\\n",
       "0      the rock is destined to be the 21st century's ...  positive   \n",
       "1      the gorgeously elaborate continuation of \" the...  positive   \n",
       "2                       effective but too-tepid biopic\\n  positive   \n",
       "3      if you sometimes like to go to the movies to h...  positive   \n",
       "4      emerges as something rare , an issue movie tha...  positive   \n",
       "...                                                  ...       ...   \n",
       "10657  a terrible movie that some people will neverth...  negative   \n",
       "10658  there are many definitions of 'time waster' bu...  negative   \n",
       "10659  as it stands , crocodile hunter has the hurrie...  negative   \n",
       "10660  the thing looks like a made-for-home-video qui...  negative   \n",
       "10661  enigma is well-made , but it's just too dry an...  negative   \n",
       "\n",
       "                                        cleaned_sentence  \n",
       "0      the rock be destine to be the 21st century  ne...  \n",
       "1      the gorgeously elaborate continuation of the l...  \n",
       "2                         effective but too tepid biopic  \n",
       "3      if you sometimes like to go to the movie to ha...  \n",
       "4      emerge as something rare an issue movie that b...  \n",
       "...                                                  ...  \n",
       "10657  a terrible movie that some people will neverth...  \n",
       "10658  there be many definition of time waster but th...  \n",
       "10659  as it stand crocodile hunter have the hurry ba...  \n",
       "10660  the thing look like a make for home video quickie  \n",
       "10661  enigma be well make but it be just too dry and...  \n",
       "\n",
       "[10662 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df = pd.read_csv('Datasets/Processed/comments_df.csv')\n",
    "comments_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-uzbekistan",
   "metadata": {},
   "source": [
    "### Word Counts & Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "unsigned-collar",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams, bigrams = [], []\n",
    "\n",
    "for index, row in comments_df.iterrows():\n",
    "    \n",
    "    comment = row['cleaned_sentence']\n",
    "    \n",
    "    if pd.isnull(comment):\n",
    "        break\n",
    "    \n",
    "    # Unigrams\n",
    "    unigrams += comment.split(' ')\n",
    "    \n",
    "    # bigrams\n",
    "    bigrams += [' '.join(bigram) for bigram in zip(comment.split(\" \")[:-1], comment.split(\" \")[1:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "convertible-instrumentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get count and store as dictionary\n",
    "unigrams_dict = pd.Series(unigrams).value_counts().to_dict()\n",
    "bigrams_dict = pd.Series(bigrams).value_counts().to_dict()\n",
    "\n",
    "# Save JSONs\n",
    "with open('Datasets/Processed/unigrams_dict.json', 'w') as write:\n",
    "    json.dump(unigrams_dict, write)\n",
    "    \n",
    "with open('Datasets/Processed/bigrams_dict.json', 'w') as write:\n",
    "    json.dump(bigrams_dict, write)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divided-namibia",
   "metadata": {},
   "source": [
    "## Stage 3 - Test & Debug"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certified-raising",
   "metadata": {},
   "source": [
    "### Calculate probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "suitable-mention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSONs\n",
    "with open('Datasets/Processed/unigrams_dict.json', 'r') as read:\n",
    "    unigrams_dict = json.load(read)\n",
    "    \n",
    "with open('Datasets/Processed/bigrams_dict.json', 'r') as read:\n",
    "    bigrams_dict = json.load(read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "democratic-musical",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unigram_prob(word):\n",
    "    \n",
    "    # Just in case\n",
    "    if word not in unigrams_dict:\n",
    "        return 1\n",
    "        \n",
    "    word_count = unigrams_dict[word]\n",
    "    unigram_size = len(unigrams_dict)\n",
    "    return word_count / unigram_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "hungarian-doctor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigram_prob(first_word, second_word, alpha, beta, gamma, coefficient):\n",
    "    \n",
    "    bigram = ' '.join([first_word, second_word])\n",
    "    \n",
    "    # Alpha prob\n",
    "    if bigram not in bigrams_dict:\n",
    "        alpha_prob = 1\n",
    "    else:\n",
    "        bigram_count = bigrams_dict[bigram]\n",
    "        alpha_prob = alpha * (bigram_count / unigrams_dict[first_word])\n",
    "    \n",
    "    # Beta prob\n",
    "    beta_prob = beta * unigram_prob(second_word)\n",
    "    \n",
    "    # coefficient\n",
    "    coef = gamma * coefficient\n",
    "    \n",
    "    return alpha_prob + beta_prob + coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "oriented-alliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_prob(sentence, alpha, beta, gamma, coefficient):\n",
    "    \n",
    "    # Clean & get sentence tokens\n",
    "    cleaned_sentence = clean_sentence(sentence)\n",
    "    words = cleaned_sentence.split(' ')\n",
    "    \n",
    "    # Calculate the first unigram prob\n",
    "    probability = unigram_prob(words[0])\n",
    "    \n",
    "    bigrams = [bigram for bigram in zip(cleaned_sentence.split(\" \")[:-1], cleaned_sentence.split(\" \")[1:])]\n",
    "    for bigram in bigrams:\n",
    "        probability *= bigram_prob(bigram[0], bigram[1], alpha, beta, gamma, coefficient)\n",
    "        \n",
    "    return round(probability * 0.5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "analyzed-shareware",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.78e-08"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_prob('why did you make me do this?', 0.4, 0.3, 0.3, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "similar-gibraltar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    \n",
    "    comment = input()\n",
    "    \n",
    "    if comment == '!q':\n",
    "        break\n",
    "    \n",
    "    prob = sentence_prob(comment, 0.4, 0.3, 0.3, 0.5)\n",
    "    print(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-simpson",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charged-insulin",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "serious-cardiff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-strength",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
